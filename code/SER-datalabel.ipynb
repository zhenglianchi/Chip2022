{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "filenames=[]\n",
    "datasets=[]\n",
    "with open(\"train.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    lines=f.readlines()\n",
    "    \n",
    "for line in lines:\n",
    "    name,data=line.split(\"\\t\")\n",
    "    filenames.append(name)\n",
    "    datasets.append(ast.literal_eval(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d3fba8b5dbc43d09\n",
      "Reusing dataset csv (D:\\hugging-face\\datasets\\csv\\default-d3fba8b5dbc43d09\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b22a24d3727435ebcd8d6398084d172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "traindata=load_dataset(\"csv\",data_files=\"datasets\\大赛1000训练用数据集.csv\")\n",
    "mark=traindata[\"train\"]\n",
    "\n",
    "markdata={}\n",
    "value={}\n",
    "name=\"001bfce2288c0cbe0a2811\"\n",
    "lastname=\"\"\n",
    "for item in mark:\n",
    "    if item[\"图名\"]!=name:\n",
    "        markdata[lastname]=value\n",
    "        value={}\n",
    "        name=item[\"图名\"]\n",
    "    value[item[\"属性名\"]]=item[\"正确值\"]\n",
    "    lastname=item[\"图名\"]\n",
    "\n",
    "markdata[mark[-1][\"图名\"]]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in markdata.items():\n",
    "    if \"入院日期\" in value.keys():\n",
    "        value[\"住院日期\"]=value[\"入院日期\"]\n",
    "        value[\"入院时间\"]=value[\"入院日期\"]\n",
    "    if \"出院日期\" in value.keys():\n",
    "        value[\"出院时间\"]=value[\"出院日期\"]\n",
    "    if \"住院天数\" in value.keys():\n",
    "        value[\"住院日数\"]=value[\"住院天数\"]\n",
    "        value[\"共住院\"]=value[\"住院天数\"]\n",
    "    if \"复核人\" in value.keys():\n",
    "        value[\"复核\"]=value[\"复核人\"]\n",
    "    if \"（小写）\" in value.keys():\n",
    "        value[\"￥\"]=value[\"（小写）\"]\n",
    "    if \"票据号码\" in value.keys():\n",
    "        value[\"No\"]=value[\"票据号码\"]\n",
    "        value[\"NO\"]=value[\"票据号码\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1=[\"入院日期\",\"住院日期\",\"入院时间\"]\n",
    "label2=[\"出院日期\",\"出院时间\"]\n",
    "label3=[\"住院天数\",\"住院日数\",\"共住院\"]\n",
    "label4=[\"复核人\",\"复核\"]\n",
    "label5=[\"（小写）\",\"￥\"]\n",
    "label6=[\"票据号码\",\"NO\",\"No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_bbox(bbox, text):\n",
    "    text=text.replace(\"\\u3000\",\"空\")\n",
    "    text=text.replace(\" \",\"空\")\n",
    "    words = list(text)\n",
    "    token_bboxes = []\n",
    "    token=[]\n",
    "    l1,r1,r2,l2 = bbox\n",
    "    x1=l1[0]\n",
    "    x2=r1[0]\n",
    "    y1=r1[1]\n",
    "    y2=r2[1]\n",
    "    unit_w = (x2-x1) / len(text)\n",
    "    for idx, word in enumerate(words):\n",
    "        curr_w = len(word) * unit_w\n",
    "        word_bbox = [[x1,y1],[x1+curr_w,y1],[x1+curr_w,y2],[x1,y2]]\n",
    "        token_bboxes.append(word_bbox)\n",
    "        token.append(word)\n",
    "        x1 += curr_w\n",
    "    \n",
    "    return token_bboxes,token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bbox(boxlist,textlist):\n",
    "    s=\"\"\n",
    "    for item in textlist:\n",
    "        s+=str(item)\n",
    "    s.replace(\"空\",\" \")\n",
    "    if boxlist==[] and s==\"\":\n",
    "        return -1,-1\n",
    "    x1=boxlist[0][0][0]\n",
    "    x2=boxlist[-1][1][0]\n",
    "    y1=boxlist[0][0][1]\n",
    "    y2=boxlist[0][2][1]\n",
    "    box=[[x1,y1],[x2,y1],[x2,y2],[x1,y2]]\n",
    "    return box,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s1,s2):\n",
    "    index=[]\n",
    "    max=0\n",
    "    data=\"\"\n",
    "    for l in s2:\n",
    "        windows=len(list(l))\n",
    "        for i in range(len(s1)-windows+1):\n",
    "            num=0\n",
    "            for j in range(windows):\n",
    "                if s1[i+j]==l[j]:\n",
    "                    num+=1\n",
    "            score=num/windows\n",
    "            if score>max:\n",
    "                index=[]\n",
    "                index.append(i)\n",
    "                index.append(i+windows-1)\n",
    "                max=score\n",
    "                data=l\n",
    "        \n",
    "    if max>=0.66:\n",
    "        return index,data\n",
    "    else:\n",
    "        return -1,-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marked={}\n",
    "for i in range(len(datasets)):\n",
    "    name=filenames[i]\n",
    "    for key,value in markdata.items():\n",
    "        if (key in name) and (name.split(\".\")[0]==key.split(\".\")[0]):\n",
    "            marked[name]=value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data/fp/label_name.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    label_list=f.read().splitlines()\n",
    "key_name=[]\n",
    "for item in label_list:\n",
    "    first=item.split(\"_\")[0]\n",
    "    if first not in key_name:\n",
    "        key_name.append(first)\n",
    "del key_name[0]\n",
    "\n",
    "key_eng=[\"GENDER\",\"AGE\",\"NAME\",\"OCODE\",\"OTYPE\",\"IN\",\"OUT\",\"DAY\",\"PC\",\"PH\",\"JY\",\"OPEN\",\"SK\",\"FH\",\"JS\",\"XX\",\"SKDW\",\n",
    "        \"YBLX\",\"HJ\",\"ZC\",\"JC\",\"HY\",\"ZL\",\"SS\",\"WSCL\",\"XY\",\"ZYYP\",\"ZCYF\",\"YBZL\",\"CW\",\"HL\",\"GH\",\"QT\",\"YBTC\",\"TC\",\n",
    "        \"XJ\",\"ZH\",\"GRZF\",\"QTZF\",\"ZFY\",\"ZFE\"]\n",
    "        \n",
    "token2label={}\n",
    "for a,b in zip(key_name,key_eng):\n",
    "    token2label[a]=b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_data/fp/label_list.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(\"OTHER\"+\"\\n\")\n",
    "    for key,value in token2label.items():\n",
    "        f.write(value+\"_KEY\\n\")\n",
    "        f.write(value+\"_VALUE\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key2(dct, value):\n",
    "    return [k for (k,v) in dct.items() if v == value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f22e03d6b34b5c8664b056ac1599cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "result={}\n",
    "bar=tqdm(range(len(datasets)))\n",
    "\n",
    "for i in range(len(datasets)):\n",
    "    name=filenames[i]\n",
    "    temp=[]\n",
    "    result[name]=temp\n",
    "    ziduan=list(marked[name].keys())\n",
    "    data=list(marked[name].values())\n",
    "\n",
    "    link={}\n",
    "    for a,b in zip(ziduan,data):\n",
    "        link[a]=-1\n",
    "        link[b]=-1\n",
    "    \n",
    "    num=0\n",
    "    for j in range(len(datasets[i])):\n",
    "        num+=1\n",
    "        dic={\"transcription\":\"\",\"label\":\"\",\"points\":\"\",\"id\":\"\",\"linking\":[]}\n",
    "        index1=-1\n",
    "        index2=-1\n",
    "        index1,p=compare(list(datasets[i][j][1][0]),ziduan)\n",
    "        index2,q=compare(list(datasets[i][j][1][0]),data)\n",
    "        r=\"\"\n",
    "        if p in label1:\n",
    "            r=label1[0]\n",
    "        elif p in label2:\n",
    "            r=label2[0]\n",
    "        elif p in label3:\n",
    "            r=label3[0]\n",
    "        elif p in label4:\n",
    "            r=label4[0]\n",
    "        elif p in label5:\n",
    "            r=label5[0]\n",
    "        elif p in label6:\n",
    "            r=label6[0]\n",
    "        else:\n",
    "            r=p\n",
    "\n",
    "        if index1!=-1 and index2!=-1:\n",
    "            boxx,tokenn=split_bbox(datasets[i][j][0], datasets[i][j][1][0])\n",
    "            if \"：\" in datasets[i][j][1][0] or \":\" in datasets[i][j][1][0]:\n",
    "                if boxx[index1[1]+2:]==[] or tokenn[index1[1]+2:]==[]:\n",
    "                    dic[\"transcription\"]=datasets[i][j][1][0]\n",
    "                    dic[\"label\"]=\"other\"\n",
    "                    dic[\"points\"]=datasets[i][j][0]\n",
    "                    dic[\"id\"]=num\n",
    "                    result[name].append(dic)\n",
    "                    continue\n",
    "                qbox,qtext=merge_bbox(boxx[index1[0]:index1[1]+2],tokenn[index1[0]:index1[1]+2])\n",
    "                abox,atext=merge_bbox(boxx[index2[0]:index2[1]+1],tokenn[index2[0]:index2[1]+1])\n",
    "                if qbox==-1 or abox==-1:\n",
    "                    dic[\"transcription\"]=datasets[i][j][1][0]\n",
    "                    dic[\"label\"]=\"other\"\n",
    "                    dic[\"points\"]=datasets[i][j][0]\n",
    "                    dic[\"id\"]=num\n",
    "                    result[name].append(dic)\n",
    "                    continue\n",
    "                dic[\"transcription\"]=qtext\n",
    "                dic[\"label\"]=token2label[str(r)].lower()+\"_key\"\n",
    "                dic[\"points\"]=qbox\n",
    "                dic[\"id\"]=num\n",
    "                link[p]=num\n",
    "                result[name].append(dic)\n",
    "                num+=1\n",
    "                dic={\"transcription\":\"\",\"label\":\"\",\"points\":\"\",\"id\":\"\",\"linking\":[]}\n",
    "                dic[\"transcription\"]=atext\n",
    "                dic[\"label\"]=token2label[str(r)].lower()+\"_value\"\n",
    "                dic[\"points\"]=abox\n",
    "                dic[\"id\"]=num\n",
    "                link[q]=num\n",
    "                result[name].append(dic)\n",
    "            else:\n",
    "                if boxx[index1[1]+1:]==[] or tokenn[index1[1]+1:]==[]:\n",
    "                    dic[\"transcription\"]=datasets[i][j][1][0]\n",
    "                    dic[\"label\"]=\"other\"\n",
    "                    dic[\"points\"]=datasets[i][j][0]\n",
    "                    dic[\"id\"]=num\n",
    "                    result[name].append(dic)\n",
    "                qbox,qtext=merge_bbox(boxx[index1[0]:index1[1]+1],tokenn[index1[0]:index1[1]+1])\n",
    "                abox,atext=merge_bbox(boxx[index2[0]:index2[1]+1],tokenn[index2[0]:index2[1]+1])\n",
    "                dic[\"transcription\"]=qtext\n",
    "                dic[\"label\"]=token2label[str(r)].lower()+\"_key\"\n",
    "                dic[\"points\"]=qbox\n",
    "                dic[\"id\"]=num\n",
    "                link[p]=num\n",
    "                result[name].append(dic)\n",
    "                num+=1\n",
    "                dic={\"transcription\":\"\",\"label\":\"\",\"points\":\"\",\"id\":\"\",\"linking\":[]}\n",
    "                dic[\"transcription\"]=atext\n",
    "                dic[\"label\"]=token2label[str(r)].lower()+\"_value\"\n",
    "                dic[\"points\"]=abox\n",
    "                dic[\"id\"]=num\n",
    "                link[q]=num\n",
    "                result[name].append(dic)\n",
    "        elif index1!=-1:\n",
    "            dic[\"transcription\"]=datasets[i][j][1][0]\n",
    "            dic[\"label\"]=token2label[str(r)].lower()+\"_key\"\n",
    "            dic[\"points\"]=datasets[i][j][0]\n",
    "            dic[\"id\"]=num\n",
    "            link[p]=num\n",
    "            result[name].append(dic)\n",
    "        elif index2!=-1:\n",
    "            if \"医院名称\" in ziduan and q==marked[name][\"医院名称\"]:\n",
    "                dic[\"transcription\"]=datasets[i][j][1][0][index2[0]:index2[1]+1]\n",
    "                dic[\"label\"]=\"name\"+\"_value\"\n",
    "                dic[\"points\"]=datasets[i][j][0]\n",
    "                dic[\"id\"]=num\n",
    "                link[q]=num\n",
    "                result[name].append(dic)\n",
    "                continue\n",
    "            \n",
    "            r=\"\"\n",
    "            f=get_key2(marked[name],q)\n",
    "            if f[0] in label1:\n",
    "                r=label1[0]\n",
    "            elif f[0] in label2:\n",
    "                r=label2[0]\n",
    "            elif f[0] in label3:\n",
    "                r=label3[0]\n",
    "            elif f[0] in label4:\n",
    "                r=label4[0]\n",
    "            elif f[0] in label5:\n",
    "                r=label5[0]\n",
    "            elif f[0] in label6:\n",
    "                r=label6[0]\n",
    "            else:\n",
    "                r=f[0]\n",
    "            dic[\"transcription\"]=datasets[i][j][1][0][index2[0]:index2[1]+1]\n",
    "            dic[\"label\"]=token2label[str(r)].lower()+\"_value\"\n",
    "            dic[\"points\"]=datasets[i][j][0]\n",
    "            dic[\"id\"]=num\n",
    "            link[q]=num\n",
    "            result[name].append(dic)\n",
    "        else:\n",
    "            dic[\"transcription\"]=datasets[i][j][1][0]\n",
    "            dic[\"label\"]=\"other\"\n",
    "            dic[\"points\"]=datasets[i][j][0]\n",
    "            dic[\"id\"]=num\n",
    "            result[name].append(dic)\n",
    "    \n",
    "    \n",
    "    list1=list(link.keys())\n",
    "    for item in list1:\n",
    "        if item in ziduan:\n",
    "            if link[item]!=-1 and link[marked[name][item]]!=-1:\n",
    "                index=[link[item],link[marked[name][item]]]\n",
    "                result[name][link[item]-1][\"linking\"].append(index)\n",
    "                result[name][link[marked[name][item]]-1][\"linking\"].append(index)\n",
    "\n",
    "\n",
    "    for v in range(len(result[name])):\n",
    "        first=result[name][v][\"label\"].split(\"_\")[0]\n",
    "        if first in label1:\n",
    "            last=result[name][v][\"label\"].split(\"_\")[1]\n",
    "            result[name][v][\"label\"]=token2label[label1[0]].lower()+\"_\"+last\n",
    "        if first in label2:\n",
    "            last=result[name][v][\"label\"].split(\"_\")[1]\n",
    "            result[name][v][\"label\"]=token2label[label2[0]].lower()+\"_\"+last\n",
    "        if first in label3:\n",
    "            last=result[name][v][\"label\"].split(\"_\")[1]\n",
    "            result[name][v][\"label\"]=token2label[label3[0]].lower()+\"_\"+last\n",
    "        if first in label4:\n",
    "            last=result[name][v][\"label\"].split(\"_\")[1]\n",
    "            result[name][v][\"label\"]=token2label[label4[0]].lower()+\"_\"+last\n",
    "        if first in label5:\n",
    "            last=result[name][v][\"label\"].split(\"_\")[1]\n",
    "            result[name][v][\"label\"]=token2label[label5[0]].lower()+\"_\"+last\n",
    "        if first in label6:\n",
    "            last=result[name][v][\"label\"].split(\"_\")[1]\n",
    "            result[name][v][\"label\"]=token2label[label6[0]].lower()+\"_\"+last\n",
    "\n",
    "    \n",
    "    \n",
    "    bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"train_data/fp/train/train.txt\",\"a\",encoding=\"utf-8\") as file:\n",
    "    for key,value in result.items():\n",
    "        file.write(key+\"\\t\"+json.dumps(value, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"train_data/fp/train/train.txt\",\"r\",encoding=\"utf-8\") as f1:\n",
    "    dataset=f1.readlines()\n",
    "    num=0\n",
    "    for item in dataset:\n",
    "        num+=1\n",
    "        if num%5==0:\n",
    "            f2=open(\"train_data/fp/val/val.txt\",\"a\",encoding=\"utf-8\")\n",
    "            f2.write(item)\n",
    "            f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集中抽取的测试集给放到图像集里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import shutil\\nwith open(\"train_data/fp/val/val.txt\",\"r\",encoding=\"utf-8\") as f1:\\n    data=f1.readlines()\\n    for item in data:\\n        filename=item.split(\"\\t\")[0]\\n        shutil.copy(\"datasets/train/\"+filename,\"train_data/fp/val/img/\"+filename)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "with open(\"train_data/fp/val/val.txt\",\"r\",encoding=\"utf-8\") as f1:\n",
    "    data=f1.readlines()\n",
    "    for item in data:\n",
    "        filename=item.split(\"\\t\")[0]\n",
    "        shutil.copy(\"datasets/train/\"+filename,\"train_data/fp/val/img/\"+filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b09ec625f77bf4fd762565a912b97636504ad6ec901eb2d0f4cf5a7de23e1ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
